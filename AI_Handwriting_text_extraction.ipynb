{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QdfkWqWgwuv",
        "outputId": "d69ad9b0-1de5-49dd-dae1-1f485d8b377a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 22 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (4,553 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 124935 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libarchive-dev libleptonica-dev\n",
            "The following NEW packages will be installed:\n",
            "  libarchive-dev libleptonica-dev libtesseract-dev\n",
            "0 upgraded, 3 newly installed, 0 to remove and 22 not upgraded.\n",
            "Need to get 3,743 kB of archives.\n",
            "After this operation, 16.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libarchive-dev amd64 3.6.0-1ubuntu1.3 [581 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libleptonica-dev amd64 1.82.0-3build1 [1,562 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libtesseract-dev amd64 4.1.1-2.1build1 [1,600 kB]\n",
            "Fetched 3,743 kB in 0s (13.8 MB/s)\n",
            "Selecting previously unselected package libarchive-dev:amd64.\n",
            "(Reading database ... 124982 files and directories currently installed.)\n",
            "Preparing to unpack .../libarchive-dev_3.6.0-1ubuntu1.3_amd64.deb ...\n",
            "Unpacking libarchive-dev:amd64 (3.6.0-1ubuntu1.3) ...\n",
            "Selecting previously unselected package libleptonica-dev.\n",
            "Preparing to unpack .../libleptonica-dev_1.82.0-3build1_amd64.deb ...\n",
            "Unpacking libleptonica-dev (1.82.0-3build1) ...\n",
            "Selecting previously unselected package libtesseract-dev:amd64.\n",
            "Preparing to unpack .../libtesseract-dev_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Setting up libleptonica-dev (1.82.0-3build1) ...\n",
            "Setting up libarchive-dev:amd64 (3.6.0-1ubuntu1.3) ...\n",
            "Setting up libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.1.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n",
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.25.3\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image) (11.1.0)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.17.0\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n"
          ]
        }
      ],
      "source": [
        "# Install Librarys\n",
        "!apt install tesseract-ocr\n",
        "!apt install libtesseract-dev\n",
        "!pip install pytesseract\n",
        "!pip install PyMuPDF\n",
        "!pip install pdf2image\n",
        "!pip install pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TlILuPcJX61",
        "outputId": "7d5a289d-c2c8-4d64-e02a-730c42c804a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TYUYSgj7CJkc"
      },
      "outputs": [],
      "source": [
        "import fitz  # PyMuPDF\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "aOijtXQoCJy_",
        "outputId": "195e690b-4d23-477e-da9a-b78354b5f348"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7c84fa84-e444-442f-8896-89f87695fcba\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7c84fa84-e444-442f-8896-89f87695fcba\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving CENSUS-HWR Dataset.pdf to CENSUS-HWR Dataset.pdf\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6cHPmohDOg3",
        "outputId": "37aa8d58-25c1-4691-ead4-6647e73fd273"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 22 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.6 [186 kB]\n",
            "Fetched 186 kB in 0s (1,077 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 125115 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.6_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.6) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.6) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oqwXSAdyCJ3x"
      },
      "outputs": [],
      "source": [
        "# Get the uploaded file name\n",
        "pdf_file = \"CENSUS-HWR Dataset.pdf\"\n",
        "\n",
        "# Convert PDF to images\n",
        "images = convert_from_path(pdf_file)\n",
        "\n",
        "# Save images temporarily\n",
        "image_paths = []\n",
        "for i, image in enumerate(images):\n",
        "    image_path = f\"page_{i+1}.png\"\n",
        "    image.save(image_path, \"PNG\")\n",
        "    image_paths.append(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5GaIgpXfCJ8x"
      },
      "outputs": [],
      "source": [
        "from PIL import ImageOps\n",
        "from PIL import ImageEnhance\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Preprocess each image\n",
        "for i, image_path in enumerate(image_paths):\n",
        "    image = Image.open(image_path)\n",
        "    image = ImageOps.grayscale(image)  # Convert to grayscale\n",
        "    image = ImageOps.autocontrast(image)  # Enhance contrast\n",
        "\n",
        "    # Apply binarization (thresholding)\n",
        "    threshold = 128  # Adjust this value based on your image\n",
        "    image = image.point(lambda p: p > threshold and 255)  # 255 is white, 0 is black\n",
        "\n",
        "    # Enhance contrast\n",
        "    enhancer = ImageEnhance.Contrast(image)\n",
        "    image = enhancer.enhance(2.0)  # Increase contrast by a factor of 2\n",
        "\n",
        "    image.save(image_path)  # Save the processed image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3nLoJLFCJ_J",
        "outputId": "bd0e089c-ebdb-47ee-a9a1-942625671038"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text from page_1.png:\n",
            "2305.16275v1 [ces.CV] 25 May 2023\n",
            "\n",
            "arXiv\n",
            "\n",
            "CENSUS-HWR: a large training dataset for\n",
            "offline handwriting recognition\n",
            "\n",
            "Chetan Joshi!, Lawry Sorenson!, Ammon Wolfert!, Dr. Mark Clement!, Dr.\n",
            "Joseph Price!, and Dr. Kasey Buckles?\n",
            "\n",
            "! Brigham Young University, Provo UT 84602, USA,\n",
            "clement@byu.edu\n",
            "? Department of Economics, University of Notre Dame, Notre Dame, IN 46556, USA,\n",
            "kbuckles@nd. edu\n",
            "\n",
            "Abstract. Progress in Automated Handwriting Recognition has been\n",
            "hampered by the lack of large training datascts. Nearly all research uses\n",
            "a set of small datascts that often causc modcls to overfit. We present\n",
            "CENSUS-HWR, a new dataset consisting of full English handwritten\n",
            "words in 1,812,014 gray scale images. A total of 1,865,134 handwritten\n",
            "texts from a vocabulary of 10,711 words in the English language are\n",
            "present in this collection. This datasct is intended to serve handwriting\n",
            "models as a benchmark for deep learning algorithms. This huge English\n",
            "handwriting recognition dataset has been extracted from the US 1930\n",
            "\n",
            "  \n",
            "\n",
            "and 1940 censuses taken by approximately 70,000 enumcrators cach year.\n",
            "The datasct and the traincd model with their weights are freely available\n",
            "\n",
            " \n",
            "\n",
            "to download at\n",
            "\n",
            "Keywords: Handwriting recognition - Information Extraction - Big Data\n",
            "\n",
            "1 Introduction\n",
            "\n",
            "With the advent of deep learning, researchers have made significant progress in\n",
            "handwriting recognition and transcription. The two most common dataset for the\n",
            "Handwriting Recognition (HWR) are the IAM [|] and RIMES [ ] dataset. They\n",
            "\n",
            "both contain Latin characters with English and French handwritten sentences\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "respectively. Although these datasets have been useful in creating handwriting\n",
            "models, additional training data is necessary to create more accurate models that\n",
            "can be generalized to more diverse handwritten documents. Limited training\n",
            "data has resulted in very complex models that usually lack explainability and\n",
            "end up overfitting. Such complex models are hard to replicate and need large\n",
            "\n",
            "     \n",
            " \n",
            "\n",
            "GPUs to effectively train. This is not ideal for an average researcher or a student\n",
            "as usually they do not have access to such resources.\n",
            "\n",
            "Additionally, the current datasets for handwriting recognition consist of care-\n",
            "fully written text with uniform spacing and do not reflect real-world handwriting.\n",
            "\n",
            " \n",
            "\n",
            "To better train models that are robust to real-world handwriting imperfections,\n",
            "a more diverse and natural dataset is needed. This concern is met by the dataset\n",
            "developed in this research.\n",
            "\f\n",
            "\n",
            "Text from page_2.png:\n",
            "2 Authors Suppressed Due to Excessive Length\n",
            "\n",
            "Jed\n",
            "\n",
            " \n",
            "\n",
            "175\n",
            "\n",
            "18c\n",
            "\n",
            "125\n",
            "\n",
            "190\n",
            "\n",
            "Word count\n",
            "\n",
            "G75\n",
            "65S\n",
            ". |\n",
            "oO\n",
            "lane S BYUHWR:\n",
            "Dataset\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Fig. 1. A bar graph of word count comparing the CENSUS-HWR datasct (BYUHWR)\n",
            "with IAM and RIMES dataset.\n",
            "\n",
            "10000\n",
            "8000\n",
            "4000\n",
            "2008\n",
            "\n",
            "lan RIMES BYUHWR\n",
            "Dataset\n",
            "\n",
            " \n",
            "\n",
            "Vocanulary\n",
            "g\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Fig. 2. A bar graph of vocabulary size comparing the CENSUS-HWR (BYUHWR)\n",
            "datasct with IAM and RIMES datasct.\n",
            "\n",
            "2 Related Work\n",
            "\n",
            "The IAM (Institut fur Informatik und Angewandte Mathematik/Department of\n",
            "Computer Science and Applied Mathematics) dataset [ ] was created to serve\n",
            "as a basis for a variety of offline handwriting recognition tasks. This English\n",
            "Handwriting dataset was particularly useful for recognition tasks where linguis-\n",
            "tic knowledge beyond the lexicon level is used. Linguistic knowledge can be\n",
            "derived from the underlying corpus [_ ]. To create this dataset, large collections\n",
            "\n",
            "  \n",
            "\n",
            " \n",
            "\n",
            "of corpora with different appearances and contents were used. The Lancaster-\n",
            "Oslo/(LOB) [  ] collection of 500 English texts, having 2000 words was used as\n",
            "a basis for the dataset.\n",
            "\n",
            "The texts in the LOB corpus were quite diverse in nature, ranging from\n",
            "review, religion, biography, and fiction to humour, romance and love stories and\n",
            "adventure. The texts were split into fragments of about 3 to 6 sentences with at.\n",
            "\f\n",
            "\n",
            "Text from page_3.png:\n",
            "CENSUS-HWR: a large training datasct for offline handwriting recognition 3\n",
            "\n",
            " \n",
            "\n",
            "umber cf Authors\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "—\n",
            "lan RIMES BYUHWR.\n",
            "Dataset\n",
            "\n",
            "Fig. 3. A bar graph of number of authors comparing the CENSUS-HWR (BYUHWR)\n",
            "datasct with IAM and RIMES datasct.\n",
            "\n",
            "a\n",
            "\n",
            "Fig. 4. Handwriting data samples from the census images.\n",
            "\n",
            " \n",
            "\n",
            "least 50 words each which were then printed into forms and several people were\n",
            "asked to write the text printed on the forms using their everyday handwriting.\n",
            "To make the image processing part easy, the writers were asked to use rulers as\n",
            "the guiding lines with 1.5 cm space between them.\n",
            "\n",
            "   \n",
            "\n",
            "The handwritten words did not contain any compression or deformed words,\n",
            "as they were asked to stop writing if they ran out of space. The handwritten\n",
            "text was written using a ballpoint pen or pencil. These forms were scanned and\n",
            "then labelling was performed. The labels were created by copying the text of\n",
            "the forms and the line feeds were filled manually. For perfect label creation,\n",
            "some manual corrections were made such as deletion, insertion or changes in the\n",
            "text. For text extraction, the skew of the document was corrected, and then a\n",
            "\n",
            "Table 1. Comparison of number of words, vocabulary, number of authors and number\n",
            "of images/forms of CENSUS-HWR (BYUHWR) datasct with the IAM and the RIMES\n",
            "datasct..\n",
            "\n",
            " \n",
            "\n",
            "IAM |RIMES CENSUS-HWR]\n",
            "Word count 82,227 |300,000 [1,865,134\n",
            "Vocabulary size /10,841/8,110 10,711\n",
            "Image/form count|1,066 |60,000  |1,812,014\n",
            "Authors count A400 1,300 70,000\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\f\n",
            "\n",
            "Text from page_4.png:\n",
            "4 Authors Suppressed Due to Excessive Length\n",
            "\n",
            "projection method was used to find the position of horizontal lines in the form.\n",
            "With this positional information, the handwritten section was extracted. Later\n",
            "on, the handwritten text was segmented into text lines and each of the text lines\n",
            "was segmented into individual words.\n",
            "\n",
            "The French RIMES(Reconnaissanc\n",
            "de fac similES / Recognition and Indexing of handwritten documents and faxes)\n",
            "project created a training set for the handwriting recognition and document\n",
            "analysis communities [ ]. The process of creating this dataset consisted of asking\n",
            "volunteers to write mail in exchange for gift vouchers. The writers were given a\n",
            "fictional identity with their own gender and up to five scenarios one at a time.\n",
            "\n",
            "et Indexation de données Manuscrites et\n",
            "\n",
            " \n",
            "\n",
            "Each of the scenarios consisted of nine realistic themes including change\n",
            "of personal information, information request, opening/closing of the customer\n",
            "account, modification of contracts, complaints, payment difficulties, reminders,\n",
            "damage declaration and destination providers. Each page was scanned and pre-\n",
            "cisely annotated to extract the maximum information which could be useful for\n",
            "evaluation such as layout structure and textual content for transcription. 300,000\n",
            "handwritten word snippets were extracted from the letters, where each snippet\n",
            "and corresponding ground truth were generated automatically but controlled\n",
            "manually to create an accurate training set. The ground truths obtained were\n",
            "faithful even to the spelling and grammatical errors.\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "The other datasets for handwriting recognition are KOHTD [ ], BanglaLekha\n",
            "[ ] and the Kurdish dataset [ ] by Rebin M. Ahmed. The KOHTD, written in\n",
            "the Kazakh language, has 3 different scripts which are Cyrillic, Latin and Ara-\n",
            "bic providing the diverse script with around 900,000 samples. The BanglaLekha,\n",
            "written in Bengali has around 166,000 samples. Ahmed’s Kurdish dataset has\n",
            "AO,095 images written by 390 native writers. However, these datasets also suffer\n",
            "from the same issues, having fewer natural handwritten styles, fewer writers, and\n",
            "fewer training samples.\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "The datasets described above do not reflect natural handwriting. They are\n",
            "intentionally collected for the aim of handwriting recognition and document\n",
            "analysis. They contain texts that were written carefully in a straight line and\n",
            "special care was taken so the words present in a line or sentence are almost\n",
            "equidistant from each other. Real historical documents are much more messy. For\n",
            "instance, the words in real documents may not be in a straight line, might consist\n",
            "of spelling and grammatical errors, words are overwritten or crossed out and\n",
            "rewritten, the same author might write cursive for a while and then transition to\n",
            "print handwritten words, or the characters might be equidistant in the beginning\n",
            "and then congested at the end due to lack of space. The handwriting community\n",
            "will benefit from a training set that is natural, contains all these errors and is\n",
            "also large enough to avoid over-fitting. The dataset developed in this research\n",
            "meets these criteria and can be used to train models that are more robust to\n",
            "such flaws.\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\f\n",
            "\n",
            "Text from page_5.png:\n",
            "CENSUS-HWR: a large training datasct for offline handwriting recognition 5\n",
            "3 Corpus and Forms\n",
            "\n",
            "The CENSUS-HWR dataset has been extracted from the US 1910 census, 1930\n",
            "census and 1940 census. It includes entries for approximately 92 million people\n",
            "who are enumerated in 1910, 123 million people in 1930 and 132 million people in\n",
            "1940 as described in National Archives microfilm publications for their respective\n",
            "years: T6241, T626, T627. This collection, which is a part of Record Group 29\n",
            "from the Bureau of the Census, includes the 48 states as well as Alaska, Hawaii,\n",
            "American Samoa, Guam, Consular Services, Panama Canal Zone, Puerto Rico,\n",
            "and the Virgin Islands. The census can be used to identify the place of residence\n",
            "on April 1, 1930, for each person that was enumerated. A manual transcription\n",
            "of these images was created by FamilySearch [ ] [ | and Ancestry.com [ ] [ ].\n",
            "The census forms consist of large sheets with rows and columns. The sched-\n",
            "ules were arranged by states, counties, place and enumeration districts, which\n",
            "were not always filed in sequential order. The census takers were asked to record\n",
            "\n",
            " \n",
            "\n",
            "information about all the people in a household. A county was the basic enu-\n",
            "meration unit, which was divided into an enumeration district. one for each\n",
            "enumerator. Once the Census forms were completed, they were sent to the Cen-\n",
            "sus Office of the Commerce Department in Washington, D.C. 95-97% of the\n",
            "population was covered in the schedule. The information on these Federal Cen-\n",
            "suses was dependent on the informant and the care taken by the enumerator,\n",
            "and hence they are usually reliable. Some of the information in the forms may\n",
            "be incorrect or deliberately falsified.\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "   \n",
            "     \n",
            "\n",
            "     \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "|. DNPATRUAT OF COMET BUREAU OF TS CHIU eng [tote\n",
            "SIATERNTH CRNGUS OF THE UNITED STATES: 1530 meme Dek Be. iB\n",
            "‘POPULATION SCHEDULE seperate Deere FL [49\n",
            "ee  emeteny sine\n",
            "aaa oe Sees [omene Sry ae >\n",
            "meet eee Beep oe 7 :\n",
            "Saha st = a ee\n",
            "ee ne ea W | jess ees ae ree\n",
            "xe Talo] P| ELH . S814] eS\n",
            "Pinas, (Al Wei! Toos rence [Gust Femcete | 23d WHeloo «|\n",
            "eis Viete| sWforpnecs [tied fecromy | 12-4 ba |\n",
            "Pan awh slick o| a Fauml 27.04 {Ta\n",
            "Put AMS mer AbAE 5\n",
            "Pots is a Aone. «|\n",
            "Prano ‘5 o7| @] ene |\n",
            "Pocayn ss] wl ol WOKE «|\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Fig. 5. Example of US 1930 Census\n",
            "\n",
            "4 Text extraction and segmentation\n",
            "\n",
            "The handwritten texts have been extracted from the scanned census images\n",
            "using an approach that utilizes scale-invariant feature transform (SIFT) [| and\n",
            "Random sample consensus (RANSAC) [ ]. SIFT is generally used to extract the\n",
            "\n",
            " \n",
            "\f\n",
            "\n",
            "Text from page_6.png:\n",
            "6 Authors Suppressed\n",
            "\n",
            "key points of objects from\n",
            "new image by comparing e\n",
            "\n",
            "the reference images. It finc\n",
            "\n",
            "of their feature vectors. Su\n",
            "\n",
            "location, scale & orientatic\n",
            "\n",
            "better matches. Object ma\n",
            "\n",
            " \n",
            "\n",
            "with high confidence [ J. I\n",
            "image for each form type\n",
            "\n",
            " \n",
            "\n",
            "each sample to its respectiv\n",
            "\n",
            "RAD\n",
            "\n",
            "images with the lines in a\n",
            "\n",
            "  \n",
            "\n",
            "in scale or rotation between\n",
            "\n",
            "filter out outliers. [ ] Basec\n",
            "\n",
            "AC uses repeatec\n",
            "\n",
            "Due to Excessive Length\n",
            "\n",
            "reference images. The objects are recognized in a\n",
            "ach feature from the new image to key points from\n",
            "s matching features based on the Euclidean distance\n",
            "osets of key points that agree on the object and its\n",
            "m in the new image are identified to filter and find\n",
            "tches that pass all the tests are identified as correct\n",
            "n our implementation, we use a template (reference)\n",
            "where we label the points of its cells. We compare\n",
            "\n",
            "x\n",
            "\n",
            "  \n",
            "\n",
            "e template image to find the matching key points.\n",
            "\n",
            "random sub-sampling to correlate features in the\n",
            "emplate image. The method is tolerant of changes\n",
            "the template and image. RANSAC is then used to\n",
            "on its findings it becomes easy to align the images\n",
            "\n",
            " \n",
            "\n",
            "and infer the locations of cells in the table. During the segmentation process,\n",
            "\n",
            "each cell of the census is ex!\n",
            "\n",
            "Although the vast majority of census pages were segmented using this method\n",
            "\n",
            "some pages had severe deg\n",
            "\n",
            " \n",
            "\n",
            "tracted and saved to an image file for that word.\n",
            "\n",
            ",\n",
            "\n",
            "radation due to physical damage or image-scanning\n",
            "\n",
            "artefacts and hence this method could not produce segmented word snippets.\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "  \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "vsrnurnn CENSUS OF TE UNTRD BTATE. 190\n",
            "ae =)\n",
            "Pres] Hy\n",
            "sat 5\n",
            "Les wen | f =I\n",
            "segmenter\n",
            "Sl\n",
            "\n",
            "Fig. 6. The pipeline of text\n",
            "then applying handwriting re\n",
            "\n",
            "Handwriting recognizer\n",
            "\n",
            "Toolmaker\n",
            "\n",
            "extraction using segmentation on the census image and\n",
            "cognition.\n",
            "\f\n",
            "\n",
            "Text from page_7.png:\n",
            "CENSUS-HWR: a large training datasct for offline handwriting recognition 7\n",
            "\n",
            "5 Labeling\n",
            "\n",
            "Each snippet that was extracted from the image by segmentation was assigned\n",
            "a unique image identifier, the row number and the field or column. This infor-\n",
            "mation was crucial and helped in matching each snippet with the corresponding\n",
            "FamilySearch transcription. This provided us with a labeled training set of mil-\n",
            "lions of images. This is an unprecedented size for a handwriting training set.\n",
            "\n",
            "These human transcriptions of the names and dates were then used to train\n",
            "a deep learning model provided along with the CENSUS-HWR dataset. This\n",
            "automatic transcription model then generated estimated transcriptions of the\n",
            "other fields in the census. These fields include profession and other fields that\n",
            "were not transcribed by Ancestry and FamilySearch.\n",
            "\n",
            " \n",
            "\n",
            "Fig. 7. Reverse indexing where humans spot crror in transcriptions.\n",
            "\n",
            "Two tools have been developed to correct transcriptions that were automat-\n",
            "ically generated by the deep learning model. Both are novel ways to involve\n",
            "qumans in transcription and take advantage of the fact that while reading hand-\n",
            "writing is a challenging task, recognizing patterns is not. Thus, the first applica-\n",
            "ion that was developed is designed to take advantage of the natural human skill\n",
            "© notice cases that are different from those around them. The Reverse Index-\n",
            "ing Application groups images based on the transcription produced by the deep\n",
            "earning model. Up to 12 of these images are presented to the user at once with\n",
            "he transcribed version of the name at the top of the screen. This process allows\n",
            "he user to see multiple versions of the same name and identify any that look\n",
            "different from the rest of the group. All transcriptions are converted to lowercase\n",
            "etters to remove complexity from the process, which allows user to validate only\n",
            "he text of images rather than capitalization.\n",
            "\n",
            "The images that are marked by the human as likely to be incorrect are\n",
            "oaded into a second application that allows the individuals to submit a free\n",
            "form responses to correct the transcription.\n",
            "\n",
            "These two applications are used to create labeled training data with an un-\n",
            "recedented level of efficiency and accuracy. This training set will allow us to\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\f\n",
            "\n",
            "Text from page_8.png:\n",
            "8 Authors Suppressed Due to Excessive Length\n",
            "\n",
            "continue to improve the results of the deep learning model progressively accu-\n",
            "rate versions of this training set will be shared with other researchers. These\n",
            "applications also have the potential to utilize a large numbers of volunteers\n",
            "crowd-sourced citizen science projects.\n",
            "\n",
            "A version of the Reverse Indexing application is being used on tablets in the\n",
            "prison system for inmates who are willing to provide service hours in exchange\n",
            "for tablet use time. There are over 500,000 tablets in prisons that may be used\n",
            "for Reverse Indexing as this application is rolled out in more states.\n",
            "\n",
            "  \n",
            "\n",
            " \n",
            "\n",
            "as\n",
            "\n",
            "  \n",
            "\n",
            "6 Further characteristics of the dataset\n",
            "\n",
            "Unlike the intentionally curated English IAM and French RIMES dataset. which\n",
            "were very clean, correctly spaced and without distortions, the CENSUS-HWR\n",
            "dataset contains various distortions, imperfections, mistakes and errors. The im-\n",
            "ages have crossed out words, rewritten or overwritten words, spelling mistakes,\n",
            "congested words, etc. Such a diverse handwritten and natural text truly repre-\n",
            "sents the style people write in a real world setting. This real world representation\n",
            "of handwritten text with various imperfections will allow researchers to develop\n",
            "models which are more robust and can still work efficiently even with imperfect\n",
            "documents. This kind of training data is needed so that researchers can explore\n",
            "new algorithms in the realm of handwriting recognition.\n",
            "\n",
            "Note that all labels in the dataset are given in lowercase letters, due to\n",
            "the reverse indexing process. This was done to remove complexity from the\n",
            "indexing validation proc since almost all images are lowercase single words\n",
            "with only the first letter capitalized. Since the dataset is validated by crowd-\n",
            "sourced volunt: is expected that there are more errors than in professionally\n",
            "annotated date . We plan to explore systems for identifying and fixing the\n",
            "\n",
            " \n",
            "\n",
            "  \n",
            "  \n",
            "\n",
            " \n",
            "\n",
            " \n",
            " \n",
            "\n",
            "    \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "mislabeled data in future work, but the current dataset is provided as is.\n",
            "Table 2 details the composition and source of images in the dataset.\n",
            "\n",
            "Table 2. Sources and types of images in the CENSUS-HWR datasct.\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Source Type Count\n",
            "1930 Census|Surname 1,178,102\n",
            "1940 Census|Education Level]495,183\n",
            "1940 Census]Occupation 114,301\n",
            "1940 Census|Industry 24,428\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "7 Handwriting model\n",
            "\n",
            "Along with the dataset, we also provide source code and weights for a hand-\n",
            "writing model trained on the CENSUS-HWR dataset. The model architecture\n",
            "is based on Bluche et al., 2017 [ ].\n",
            "\f\n",
            "\n",
            "Text from page_9.png:\n",
            "CENSUS-HWR: a large training datasct for offline handwriting recognition 9\n",
            "\n",
            "The model takes gray scale images as input, which it resizes to 64 x 512 pixels.\n",
            "Original image aspect ratios are preserved, and padding is added as necessary.\n",
            "Six gated convolution blocks reduce the image to 1 x 64 with 512 features. Two\n",
            "bidirectional RNN blocks then map the features to the output character set. The\n",
            "provided model defaults to the same character set as Start, Follow, Read [_ ].\n",
            "The model is trained with CTC loss. Note that the provided model is limited to\n",
            "predicting 64 characters per image based on the input image size, since most of\n",
            "the dataset images are single words.\n",
            "\n",
            " \n",
            "   \n",
            "\n",
            "We trained preliminary models using 10-fold cross validation on the full\n",
            "dataset without augmentation. The training set was randomly split into train-\n",
            "ing/validation sets with an 85%-157\n",
            "\n",
            " \n",
            "\n",
            "% split. Models were trained for twelve epochs,\n",
            "and the weights with the lowest loss on the validation set were saved. Each model\n",
            "\n",
            " \n",
            "\n",
            "was then evaluated on the withheld validation section of the dataset. Table 3\n",
            "contains the results of each test.\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Table 3. Character Error Rates (CER) from cross validation tests. The highest and\n",
            "lowest crror rates are bolded.\n",
            "Section withhcld|CER\n",
            "1 4.6827%\n",
            "2 4.6726%\n",
            "3 4.6830%\n",
            "4 4.7115%\n",
            "5 4.6662%\n",
            "6 A.5915%\n",
            "7 4.6460%\n",
            "8 4.6218%\n",
            "9 4.5682%\n",
            "10 4.6346%\n",
            "[Mean 4.6478%\n",
            "We then trained a model on the full dataset under the same conditions. We\n",
            "\n",
            " \n",
            "\n",
            "estimate the character error rate (CER) of the model as the mean of the cross-\n",
            "validation tests to be 4.6478%. This model is provided as a proof of concept for\n",
            "using this dataset.\n",
            "\n",
            "8 Conclusion\n",
            "\n",
            "This paper presents the CENSUS-HWR, which provides a large and natural\n",
            "dataset that represents a diverse variety of natural handwritten styles. This\n",
            "is the largest handwriting dataset with 1.8 million handwritten samples and\n",
            "70,000 authors that was extracted from the US 1930 census and 1940 census.\n",
            "This dataset is intended to assist the handwriting recognition community in\n",
            "eveloping robust models.\n",
            "\n",
            " \n",
            "\f\n",
            "\n",
            "Text from page_10.png:\n",
            "10\n",
            "\n",
            "Authors Suppressed Due to Excessive Length\n",
            "\n",
            "While many datasets commonly referenced for handwriting recognition con-\n",
            "sist of samples with little noise, our dat\n",
            "\n",
            "t reflects the noisiness of most real-\n",
            "\n",
            "   \n",
            "\n",
            "world samples. This provides better training for models on a diversity of hand-\n",
            "writing styles. And when used in validation and test sets, provides a more rig-\n",
            "orous evaluation.\n",
            "\n",
            "References\n",
            "\n",
            "10.\n",
            "\n",
            "ll.\n",
            "\n",
            "12.\n",
            "\n",
            "14.\n",
            "\n",
            ". Ahmed, R.M., Rashid, T.A., Fatah, P., Alsadoon, A., Mirjalili, S.: An extensive\n",
            "\n",
            "dataset of handwritten central kurdish isolated characters. Data in Brief 39, 107479\n",
            "(2021)\n",
            "\n",
            ". Ancestry: 1930 united states federal census records— ancestry. https://www.\n",
            "\n",
            "ancestry.com/search/collections/6224/ (2002)\n",
            "\n",
            ". Ancestry: 1940 united states federal census records— ancestry. https://www.\n",
            "\n",
            "ancestry.com/search/collections/2442/ (2012)\n",
            "\n",
            "Biswas, M., Islam, R., Shom, G.K., Shopon, M., Mohammed, N., Momen,\n",
            "$., Abedin, M.A.: BanglaLekha-Isolated: A Comprchensive Bangla Handwrit-\n",
            "ten Character Dataset (Feb 2017), http://arxiv.org/abs/1703.10661, number:\n",
            "arXiv:1703.10661 arXiv:1703.10661 [cs]\n",
            "Bluche, T., Messina, R.: Gated convolutional recurrent neural networks for multi-\n",
            "ingual handwriting recognition. In: 2017 14th IAPR international conference on\n",
            "document analysis and recognition (ICDAR). vol. 1, pp. 646-651. IEEE (2017)\n",
            "\n",
            " \n",
            "\n",
            "FamilySearch: United states census, 1930 -  familyscarch _ historical\n",
            "records. https: //www-familysearch.org/en/wiki/ United States_Census,\n",
            "-1930_-_FamilySearch_Ilistorical_Records (2002)\n",
            "\n",
            "FamilySearch: United states census, 1940 -  familysearch _ historical\n",
            "records. https: //www-familysearch.org/en/wiki/United_States_Census,\n",
            "\n",
            "-1940_-_FamilySearch_Ilistorical_Records (2012)\n",
            "\n",
            "Fischler, M.A., Bolles, R.C.: Random sample consensus: a paradigm for model\n",
            "fitting with applications to image analysis and automated cartography. Communi-\n",
            "cations of the ACM 24(6), 381-395 (1981)\n",
            "\n",
            ". Grosicki!, E., Carre, M., Brodin, J.M., Gcoffrois', E.: Rimes evaluation campaign\n",
            "\n",
            "for handwritten mail processing. ICFHR 2008 : 11th International Conference on\n",
            "Frontiers in Handwriting Recognition pp. 1 — 6 (Aug 2008)\n",
            "\n",
            "Johansson, S., Leech, G.N., Goodluck, H.: Manual of information to accompany the\n",
            "Lancastcr-Oslo/Bergen Corpus of British English, for use with digital computer.\n",
            "Department of Eng University of Oslo (1978)\n",
            "\n",
            "Lowe, D.G.: Distinctive image features from scale-invariant keypoints. Interna-\n",
            "tional journal of computer vision 60, 91-110 (2004)\n",
            "\n",
            "farti, U.V., Bunke, H.: The iam-database: an english sentence database for of-\n",
            "fline handwriting recognition. International Journal on Document Analysis and\n",
            "Recognition 5, 39-46 (2002)\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            ". Toiganbayeva, N., Kascm, M., Abdimanap, G., Bostanbckov, K., Abdallah, A.,\n",
            "\n",
            "Alimova, A., Nurscitov, D.: Kohtd: Kazakh offlinc handwritten text dataset. Signal\n",
            "Processing: Image Communication 108, 116827 (2022)\n",
            "\n",
            "Wigington, C., Tensmeycr, C., Davis, B., Barrett, W., Price, B., Cohen, S\n",
            "follow, read: End-to-cnd full-page handwriting recognition. In: Proccedings of the\n",
            "European Conference on Computer Vision (ECCV) (September 2018)\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "  \n",
            "\f\n",
            "\n"
          ]
        }
      ],
      "source": [
        "extracted_text = []\n",
        "\n",
        "for image_path in image_paths:\n",
        "    # Open the image\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    # Perform OCR\n",
        "    text = pytesseract.image_to_string(image, lang='eng')  # Use 'eng' for English\n",
        "    extracted_text.append(text)\n",
        "\n",
        "    # Print the extracted text\n",
        "    print(f\"Text from {image_path}:\\n{text}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the extracted text to a file\n",
        "output_text_path = 'extracted_text.txt'\n",
        "with open(output_text_path, 'w', encoding='utf-8') as text_file:\n",
        "    text_file.write(text)"
      ],
      "metadata": {
        "id": "wK5-hJznjBqN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nQ4Qj7tkCKDO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "52e40867-d9e7-4a51-c5ce-3c5877e1dec5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0e128bd1-956f-4d9c-81df-9c222353bd47\", \"extracted_text.txt\", 3126)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Download the extracted text file\n",
        "from google.colab import files\n",
        "files.download(\"extracted_text.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HRp4glECKF0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-V447-vCKJn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsw_D4HPCKOC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}